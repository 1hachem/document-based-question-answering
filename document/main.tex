\documentclass[a4paper,12pt]{article}

\usepackage[a4paper,
            bindingoffset=0.2in,
            left=0.8in,
            right=0.8in,
            top=1in,
            bottom=1in,
            footskip=.25in]{geometry}

\usepackage{times}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[acronym]{glossaries}
\usepackage{multirow}
\usepackage{authblk}
\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}

\lstdefinestyle{mystyle}{ 
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\makeglossaries

\newacronym{nlp}{NLP}{Natural Language Processing}
\newacronym{gpt}{GPT}{Generative Pre-trained Transformer}
\newacronym{qa}{QA}{Question answering}
\newacronym{ml}{ML}{Machine learning}
\newacronym{ai}{AI}{Artificial intelligence}
\newacronym{llm}{LLM}{Large Language Model}
\newacronym{dbqa}{DBQA}{Document-based question answering}
\newacronym{rnn}{RNN}{Recurrent neural network}
\newacronym{lstm}{LSTM}{Long-short term memory}
\newacronym{odqa}{ODQA}{Open-domain question answering}
\newacronym{ir}{IR}{Information retrieval}
\newacronym{abc}{ABC}{Asset-based consulting}

\author[1,2]{Hachem Betrouni}
\affil[1]{National Polytechnic school of Algiers, Industrial Engineering Department, Data Science and AI, Algiers, hachem.betrouni@g.enp.edu.dz}
\affil[2]{BIGmama technology, France, hb@big-mama.io}

\title{Document-based question answering}

\begin{document}


\maketitle
\begin{abstract}
	In this thesis, we present a comprehensive analysis of various open-source embedding models and large language models (LLMs) for document-based question answering (DBQA). The goal of DBQA is to extract relevant information from a given document and answer user queries in natural language. We evaluate the performance of different embedding models, including BERT, E5, and MiniLM, in combination with open source LLMs such as Vicuna, Falcon, and OpenLlama.
	Our experimental setup involves a retriever-generator framework, where a retrieval system retrieves the most relevant contexts from a document using embeddings of document and query. Then we condition the generative system (LLM) with the most promising context to generate a response to the user. 
	We benchmark the performance of these systems using a sample from the SQuAD dataset and GPT-3 as a judge.
	We find that the choice of embedding models significantly impacts the retrieval performance. Moreover, we observe that the choice of LLM also plays a crucial role in generating accurate and informative answers.
	This comparative analysis provides insights into the strengths and weaknesses of various open-source embedding models and LLMs for DBQA tasks. 
	The findings of this study can guide researchers and practitioners in selecting suitable models for their specific DBQA applications and contribute to the advancement of question-answering systems.
\end{abstract}

\clearpage
\begin{center}
	\thispagestyle{empty}
	\vspace*{\fill}
	To my parents,\\
	to my family, mentors, friends, enemies, and every person who shaped who I am, \\
	to every rock that broke my bones, to every sea waving on the shores, \\  
	to Algiers's seagulls waking me up morning, reminded of Allah's boons,\\
	to Collo's men who rescued me,\\
	to you all I dedicate not this thesis but the years that led to its maketh.\\
	\vspace*{\fill}
\end{center}
\clearpage


\onecolumn
\tableofcontents
\listoffigures
\listoftables

\newpage
\printglossary[type=\acronymtype, title=List of acronyms]

\newpage

\section{General introduction}
For this BIGmama confined the development of a document-based question answering model to us.

\begin{figure}[h]
    \begin{center}
      \includegraphics[width=0.4\textwidth]{figures/plan.png}
    \end{center}
  \caption{General structure of this document.}
    \label{fig:plan}
\end{figure}

\section{State of play}
\subsection{Introduction}
Creating value in the market is usually about solving a complex problem or answering a question. When it comes to the Data science and AI sector this used to be tackled with off-the-shelf solutions/applications/answers.
However the arrival of LLMs and other fundamental models upset the level of requirements. Creating value now requires expensive,  bespoke AI solutions. 
The requirements are no longer to predict a number, but to explain the prediction, it's no longer to recommend a blog title but to generate its whole content.   

\subsection{Asset-based consulting}
This evolvement of the business landscape, driven by rapid technological advancements, changing market dynamics, and increasing competition created a fast-paced environment, where organizations seek innovative ways to optimize their operations, unlock hidden potentials, and achieve sustainable growth. 

Today we are well aware that answering these organizations needs for innovation has shifted from writing consulting reports (or any of the traditional approaches) to building tangible assets that concretize the added value. Today, clients are expecting specialised solutions to their problems, rather than the boilerplate solutions consultants traditionally provided.

This has given rise to a thriving industry known as asset-based consulting (ABC).

Asset-based consulting is a specialized field within consulting that focuses on creating, leveraging, and maximizing the value of a company's assets, 
AI can be incorporated in the schema of consulting by leveraging the data/knowledge/expertise of the company, incorporating it with intelligent systems, 
automatize tasks and improve the productivity of its users in general. 

In the ABC context, assets can include tangible resources such as infrastructure, 
technology, and inventory, as well as intangible assets like intellectual property, brand equity, and human capital. 
By harnessing the full potential of these assets, asset-based consultants help organizations drive operational efficiency, 
enhance performance, and create a competitive advantage.

Unlike traditional consulting approaches that often emphasize external factors and strategies, 
asset-based consulting takes a more holistic and internal perspective. 
It recognizes that companies possess unique assets and capabilities that, when strategically managed, or in our case, combined with AI, 
can serve as the foundation for sustainable competitive advantage and resilience. 
Here consultants work closely with clients to understand their expertise, identify untapped questions and needs, and design tailored AI solutions.

Asset-based consulting is known for its multidisciplinary nature, drawing expertise from various fields including finance, 
operations, technology, marketing, and human resources. 

Consultants blend their industry knowledge, analytical skills, and business acumen to assess, 
diagnose, and unlock the hidden value with innovative assets. They often conduct thorough assessments, employ data-driven methodologies, and collaborate closely with expert teams 
to align the asset usefulness with the overall business objectives.

As industries become increasingly complex and competitive, the demand for asset-based consulting continues to rise. 
Organizations of all sizes and sectors recognize the need to unlock the value with AI assets to stay ahead of the curve. 
From manufacturing firms seeking to optimize their supply chains to technology companies aiming to capitalize on their intellectual property, 
asset-based consulting provides a strategic framework and expertise to achieve these objectives.

\subsection{BIGmama technology}
BIGmama is a French registered startup with an Algerian extension, specialized in data science and AI. 
They have been developing bespoke predictive applications for more than 8 years (as of the year 2023).

The board of directors counts former CEOs of global groups (Danone, Safran) and the team is assembled from a dozen of high-level data scientists 
and software engineers.

\subsection{Mission}
BIGmama's mission is to democratize the access to bespoke AI applications, to transform companies and individuals by introducing AI in their jobs however possible. 

From an original method based on the know-how developed while working with their partners for nearly a decade, 
they are able of identifying and extracting expertise as well as combine it with machine learning system (Hybrid AI).

What is important at BIGmama today is to be able to industrialize this methodology by proposing a software (HYKO) that automatize the extraction of expertise and 
its hybridization with machine learning models. In this way, this tool will allows them to build state of the art predictive applications, relatively quickly and at an affordable cost.

They believe that a large part of the future of the AI sector will be played out in the ability to articulate human expertise with the countless possibilities offered by AI models.

\subsection{Vision}
Data science will soon become a commodity, the arrival of large language models (which are considered foundational models in NLP) mark but a start of a new wave of foundational models. In the near future, we can expect the emergence of generalist models that are equivalent to ChatGPT but specialized in computer vision tasks, forecasting, pattern recognition, and other domains.

AI projects that take 6 months in the making, require extensive "hyper-parameter tuning", hundreds of experiments, task specific architectural modeling, and that come at an expensive price will soon become obsolete and outdated.

The future is going to become in the hands of those who are capable of harnessing the power of these generalist models. Algeria and the whole African continent is far behind, and its no longer a question of closing this gap step by step, a "quantum jump" is required. BIGmama will lead this quantum jump, and will prove that excellence can beam from within the African continent. 

\subsection{Unique methodology}
One of the valuable heritages that BIGmama acquired during the 8 years of actively developing bespoke AI applications to its clients is a the unique methodology of work at BIGmama. 
This methodology is centered around the following ideas : 

\subsubsection{AI starts with problematization.}
Bespoke AI solutions development does not start with AI but with a re-framing and a "problematization" work. 
Clients most often arrive with a subject (and not problems). A subject in it self is composed of many "hidden" problems (often hidden even to the client), 
a good consultant at BIGmama knows that the process of "problematization" (that of decomposing the subject into multiple problems) requires multiple iterations and attack angles, 
and that its success is deeply rooted to that of understanding the expertise of the clients, their needs and to the skills of the consultant to translate these problems into something addressable 
by the current AI models.

\subsubsection{AI is a tool, not an end-goal}
BIGmama firmly believes that AI should be seen as a powerful tool rather than an ultimate goal in itself. While AI technologies continues to advance rapidly, 
its true value lies in its ability to address and solve real-world problems and challenges faced by individuals, businesses, and society as a whole.
By recognizing AI as a tool, BIGmama emphasize its role in enabling and enhancing human capabilities rather than replacing them, and emphasis that the current 
approaches in AI are not always the best way to solve \textit{every} problem.

\subsubsection{Hybridization}
The future of AI lies in what is commonly referred to today as hybrid AI. 
This is a set of approaches and methodologies aiming at combining the potential of models with purely human knowledge. 
This hybridization allows BIGmama to put humans at the heart of technological development and to produce tools that are more efficient, easier to maintain, 
explain and that are far less expensive.

\subsection{Software as a service (SaaS)}
BIGmama is currently working on a software that intrinsically embeds its mission of democratizing AI and its unique methodology of how to do so. 
Hyko is an iterative software that help users formulate their problems properly using AI conversational systems powered by LLMs. 
It then automatically generate working applications that combines and orchestrate multiple AI models predisposed in a model-base.

Hyko can be described as a three steps iterative process :

\paragraph{Diagnosis and problematization (Scoping)}
As discussed earlier developing a bespoke AI application starts first by identifying the needs and sub-problems of your clients. 
Folks at BIGmama were able to identify the most important questions to ask your client in order to reach a common understanding of what can be done and how.
These questions cover topics ranging from ethics and governance, success conditions, explainability of the solution, to empirical rules discovered by the experts.

Today this process of scoping is automatized with an intelligent chatbot in Hyko. 

\paragraph{Automatic prototyping}
Hyko leverages the ever increasing availability of open-source AI models (Huggingface \cite{huggingface}, Github, etc.)
and the zero-shot reasoning capabilities \cite{zeroshot} of today's LLMs to generate a first prototype solution based on the output of the scoping phase.  

In order to design and execute this prototype we need first to solve the following two tasks : 

\begin{enumerate}
	\item task planning : designing how the sub problems interact and should be addressed, setting up the inputs, outputs and the dependencies of each problem (task) in the pipeline.
	\item model selection : selecting the right model (available in the model base) to tackle each problem in the pipeline.
\end{enumerate}

Similar to HuggingGPT \cite{hugginggpt}, by solving these tasks Hyko is capable of going from a scoping of the user's problems to 
an executable AI prototype solution fully automatically.


\paragraph{Enhancement of the prototype}
The last step involves further customizing and enhancing the prototype to the exact needs of the client.
Following this framework Building tailor-made AI solution will become cheaper faster and accessible to everyone, requiring minimal (to none) technical intervention.


\begin{figure}[h]
	\centering
	\includegraphics[width=.5\linewidth]{figures/3process.png}
	\caption{Iterative process of developing an AI asset in Hyko.}
	\label{fig:hykoprocess}
\end{figure}

\subsection{Knowledge representation}
A very important aspect of the first step (Scoping) is the ability to represent already existing knowledge (e.g. technical documents) 
and make it digestible by Hyko chatbot. 

This allows the chatbot to use necessary information and guide the conversation effectively.

At BIGmama we experimented with various techniques and methods for representing existing body of knowledge, 
first approaches involved working with ontologies and knowledge graphs :

\begin{enumerate}
	\item Ontologies: Ontologies provide a structured way to represent knowledge by defining concepts, relationships, and properties within a specific domain. They establish a common vocabulary and enable reasoning about the knowledge. 
	      Ontologies are often represented using ontology languages like RDF (Resource Description Framework) or OWL (Web Ontology Language).
	\item Knowledge graphs: Knowledge graphs organize knowledge as a network of interconnected nodes representing entities, concepts, or facts. 
	      These graphs capture not only the relationships between entities but also their attributes and contextual information. Knowledge graphs can be built using graph databases or triple stores.
\end{enumerate}

However with the last advances in language modeling and information retrieval systems, document-based question answering become a more appealing option.  


\subsubsection{Document-based question answering}
Document-based question answering (DBQA) is a machine learning task that focuses on extracting accurate and relevant answers from a given document and responding in natural language to a specific query. 

It involves representing and manipulating content within textual documents, such as articles, technical documentation in order to provide precise answers to user's queries.

DBQA systems typically involve a two stages process : 
\begin{enumerate}
	\item context retrieval : employing information retrieval (IR) techniques to extract relating context passages from the document.
	\item question answering (QA) : using natural language processing (NLP) approaches to answer a query using the retrieved context.  
\end{enumerate}  

\subsection{Conclusion}
In this paper we will dive deeper into the different frameworks, models and techniques used in DBQA systems (and similar QA tasks). 
We experimented further with different DBQA retriever-generator settings, we prepared a small benchmark for few open-source LLMs used (in combinations) 
with context retrieval systems.

\clearpage
\section{State of the art}

\subsection{Introduction}
Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. 
It involves the development of algorithms and models that enable computers to understand, interpret, and generate human language in a meaningful way.

One fundamental aspect of NLP is the representation of words as dense vectors in a high-dimensional space, known as embeddings. 
Word embeddings aim to capture the semantic and syntactic relationships between words, enabling algorithms to process language more effectively. 
These embeddings can be created using neural or statistical approaches.

Transformers, introduced by "attention is all you need" \cite{attention}, are a type of deep learning model that revolutionized NLP. 
Transformers utilize attention mechanisms, which allow the model to attend to different parts of the input sequence when processing each element. 
The encoder-decoder architecture extends the transformer model to handle tasks with varying input and output sequence lengths.

As we will see NLP plays a crucial role in tackling the DBQA task, but first we will dive further into the state-of-the-art of the different embedding models 
and transformer architectures available in the litterateur. We will further explore the different frameworks used to solve similar tasks to DBQA.  




\subsubsection{Transformers}

Transformers are a type of deep learning model that have revolutionized natural language processing (NLP) tasks. They were introduced by \cite{attention} and have since become the state-of-the-art approach for various NLP applications.

\textbf{Attention Mechanism:}

Attention mechanisms are a key component of transformers. They allow the model to focus on different parts of the input sequence when processing each element. The attention mechanism assigns weights to each element in the input sequence based on its relevance to the current element being processed.

Given an input sequence $\mathbf{X} = \{x_1, x_2, \ldots, x_n\}$, the attention mechanism computes a weighted sum of all elements in $\mathbf{X}$, where the weights are determined by the relevance of each element to the current element. This can be mathematically represented as:

$$Attention(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = softmax\left(\frac{\mathbf{QK}^T}{\sqrt{d_k}}\right) \mathbf{V}$$

where $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are the query, key, and value matrices, respectively, and $d_k$ represents the dimension of the key.

\textbf{Multi-Head Attention:}

To capture different types of information, self-attention is performed multiple times in parallel, with different learned linear projections. The outputs of these attention heads are concatenated and linearly transformed:

$$MultiHead(X) = Concat(head_1, head_2, ..., head_h)W_O$$

where $head_i = SelfAttention(XW_{Qi}, XW_{Ki}, XW_{Vi})$, $W_{Qi}$, $W_{Ki}$, and $W_{Vi}$ are learnable weight matrices, and $W_O$ is the output weight matrix.

\textbf{Encoder Transformer:}

In a transformer model, the encoder is responsible for encoding the input sequence. It consists of a stack of identical layers, each comprising a multi-head self-attention mechanism and a feed-forward neural network. The encoder processes the input sequence in parallel and captures the dependencies between different elements.

Let $\mathbf{X} = \{x_1, x_2, \ldots, x_n\}$ be the input sequence. The encoder takes $\mathbf{X}$ and produces a sequence of encoded representations $\mathbf{Z} = \{z_1, z_2, \ldots, z_n\}$. The encoding process can be expressed as:

$$\mathbf{Z} = Encoder(\mathbf{X}) = EncoderLayer(\ldots(EncoderLayer(\mathbf{X})))$$

where $EncoderLayer$ represents a single layer in the encoder, and the ellipsis denotes the stacking of multiple layers.

\textbf{Encoder-Decoder Architecture:}

The encoder-decoder architecture extends the transformer model to tasks such as machine translation, where the input and output sequences have different lengths. The encoder processes the input sequence, while the decoder generates the output sequence.

Let $\mathbf{X} = \{x_1, x_2, \ldots, x_m\}$ and $\mathbf{Y} = \{y_1, y_2, \ldots, y_n\}$ be the input and output sequences, respectively. The encoder-decoder architecture can be summarized as:

$$\mathbf{Z} = Encoder(\mathbf{X})$$
$$\mathbf{Y'} = Decoder(\mathbf{Z}, \mathbf{Y}) = DecoderLayer(\ldots(DecoderLayer(\mathbf{Z}, \mathbf{Y})))$$

where $Decoder$ represents the decoder, $\mathbf{Z}$ is the encoded representation of $\mathbf{X}$ obtained from the encoder, and $\mathbf{Y'}$ is the predicted output sequence.

The decoder processes the output sequence $\mathbf{Y}$ and attends to the encoded representation $\mathbf{Z}$ to generate the output. The decoder also employs self-attention, allowing it to attend to previously generated elements in the output sequence.

\textbf{Decoder Transformer}
The decoder transformer is an essential component of the transformer model for text generation tasks. It operates in an autoregressive manner, generating one token at a time. Given an encoded representation $Z$ obtained from the encoder, 
the decoder attends to $Z$ and utilizes self-attention mechanisms to capture contextual dependencies.

Writers of \cite{formaltransformer} gives a good formulation for the decoder transformer task : 

Given a vocabulary $\mathcal{V}$, let $x_n \in \mathcal{V}$ for $n \in [N_{data}]$ be a dataset of sequences (imagined to be) sampled i.i.d. from some distribution $P$ over $\mathcal{V}$. 
The goal is to learn an estimate $\hat{P}$ of the distribution $P(x)$. 
In practice, the distribution estimate is often decomposed via the chain rule as $\hat{P}(x) = \hat{P}{\theta}(x[1]) \cdot \hat{P}{\theta}(x[2] ,|, x[1]) \cdot \ldots \cdot \hat{P}_{\theta}(x[T] ,|, x[1:T-1])$, 
where $\theta$ consists of all neural network parameters to be learned. 
The goal is to learn a distribution over a single token $x[t]$ given its preceding tokens $x[1:t-1]$ as context.

By iteratively generating each token while considering the context, the decoder transformer produces high-quality and coherent text. 
It is widely used in various applications such as machine translation, text summarization, and question answering.




\subsubsection{Embeddings}
A word embedding is a technique used in NLP to represent words as dense vectors in a high-dimensional space (an embedding). It aims to capture the semantic and syntactic relationships between words, enabling algorithms to process language more effectively.

Word embeddings can be created using two main approaches (or a combination of both): neural and statistical. 
Neural approaches, use neural networks to learn word representations by capturing semantic and syntactic relationships. Statistical approaches, rely on statistical features of words and their occurrences.

\paragraph{TF-IDF}
TF-IDF (Term Frequency-Inverse Document Frequency) is a commonly used weighting scheme in information retrieval and text mining tasks, designed to capture the importance of terms in a document collection. It combines two fundamental concepts: term frequency (TF) and inverse document frequency (IDF). TF refers to the number of times a term appears in a document, normalized by the total number of terms in that document. IDF, on the other hand, quantifies the rarity of a term across the entire document collection by taking the logarithm of the inverse of its document frequency. 

Document frequency refers to the number of documents in the collection that contain a given term. The TF-IDF weight for a term in a document is obtained by multiplying its TF value by its IDF value. This approach assigns higher weights to terms that appear frequently in a specific document but are relatively rare across the entire collection, effectively capturing their discriminative power. the TF-IDF weight of a term $t$ in a document $d$ can be computed as follows:

$${\displaystyle \mathrm {tf} (t,d)={\frac {f_{t,d}}{\sum _{t'\in d}{f_{t',d}}}}}$$
$$ \mathrm{idf}(t, D) =  \log \frac{N}{|\{d \in D: t \in d\}|}$$
$${\displaystyle \mathrm {tfidf} (t,d,D)=\mathrm {tf} (t,d)\cdot \mathrm {idf} (t,D)}$$


where $f_{t,d}$ is the raw count of a term in a document, i.e., the number of times that term $t$ occurs in document $d$,
$N$ the total number of documents in the corpus $N=|D|$ and  $|\{d \in D: t \in d\}|$ is the number of documents where term $t$ apears.

The resulting TF-IDF weights can be used for various purposes, such as document ranking, text classification, or keyword extraction, enabling the identification of important terms that characterize the content of documents in a collection.

\paragraph{Bert}

BERT \cite{bert}, which stands for Bidirectional Encoder Representations from Transformers, is a pre-trained natural language processing model introduced by Google \cite{bert}.
The key idea behind BERT is to train a deep bidirectional representation of language by pre-training a language model on a large corpus of unlabelled text. This pre-training step is done by predicting missing words within sentences, a task known as masked language modeling. 
BERT also incorporates a next sentence prediction task, where the model learns to predict whether two sentences appear consecutively in the training corpus.
After pre-training, BERT is fine-tuned on specific downstream NLP tasks such as text classification, named entity recognition, and question-answering, among others. 
Fine-tuning involves training the model on a smaller labeled dataset specific to the downstream task, which allows BERT to learn task-specific features.
One of the main advantages of BERT is its ability to capture the contextual meaning of words by considering both the left and right context. 
This bidirectional approach makes BERT particularly effective for tasks that require an understanding of the relationships between words in a sentence, resulting in contextual word embeddings that capture semantic and syntactic relationships.

However BERT \cite{bert} uses a cross-encoder (i.e. two sentences are passed to the transformer network and the target value is predicted). This makes it unsuitable for various pair regression tasks due to too many possible combinations (finding most similar pair in a collection of $n$ sentences requires $\frac{n*(n-1)}{2}$ inferences). Sentence-BERT (SBERT) \cite{sbert}, is a modification of the BERT network using siamese and triplet networks, SBERT is able to
derive semantically meaningful sentence embeddings (semantically similar sentences are close in vector space).

\paragraph{E5}
E5 \cite{e5} is trained using a contrastive learning approach with weak supervision signals from a curated large-scale text pair dataset. 
E5 serves as a versatile embedding model suitable for various tasks that require a single-vector representation of texts, such as retrieval, clustering, and classification. 
According to Huggingface embedding models leaderboard \cite{embedding-leaderboard}, the E5 \cite{e5} is the current stat-of-the-art embedding model (average performance across 62 datasets and more than 7 tasks).

\paragraph{MiniLM}
MiniLM \cite{minilm} uses a simple and effective knowledge distillation method to compress large pre-trained Transformer based language models. The student model (MiniLM) is trained by deeply mimicking the teacher’s self-attention \cite{attention} modules,
which are the vital components of the Transformer networks.

The authors of \cite{minilm} propose using the self-attention distributions and value
relation of the teacher’s last Transformer layer to guide the
training of the student, which is effective and flexible for the student models. 

The embeddings that come from this model are of high quality, cheap to compute and can be used in a number of down stream tasks.


\subsubsection{Similarity measures}
Similarity measures are used to quantify the degree of similarity or dissimilarity between two objects or vectors. They are widely used in various domains, including information retrieval, recommendation systems, and clustering. Different similarity measures capture different aspects of similarity based on the specific requirements of the task.

Here we mention a few examples of similarity measures commonly used in the litterateur. However the selection of a suitable similarity measure is contingent upon the characteristics of the data and the particular objectives of the task being performed.


\textbf{Cosine Similarity:}

Cosine similarity is a commonly used similarity measure that computes the cosine of the angle between two vectors. It is particularly useful when comparing the similarity between documents, text embeddings, or high-dimensional data. The cosine similarity between two vectors $\mathbf{A}$ and $\mathbf{B}$ can be calculated as:

$$cosine\_similarity(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|}$$

where $\cdot$ denotes the dot product and $\|\cdot\|$ represents the Euclidean norm.

\textbf{Euclidean Distance:}

Euclidean distance is a commonly used dissimilarity measure that calculates the straight-line distance between two points in Euclidean space. It is widely used in clustering algorithms, such as k-means. The Euclidean distance between two vectors $\mathbf{A}$ and $\mathbf{B}$ of the same dimension can be computed as:

$$
euclidean\_distance(\mathbf{A}, \mathbf{B}) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}
$$

where $A_i$ and $B_i$ represent the $i$-th elements of vectors $\mathbf{A}$ and $\mathbf{B}$, respectively.

\textbf{Jaccard Similarity:}

Jaccard similarity \cite{jaccard} is a measure commonly used for comparing the similarity between sets. It is particularly useful in text mining and recommendation systems. The Jaccard similarity between two sets $A$ and $B$ is calculated as the size of their intersection divided by the size of their union:

$$
jaccard\_similarity(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$

where $|A|$ and $|B|$ denote the cardinalities of sets $A$ and $B$, respectively.

\textbf{Hamming Distance:}

Hamming distance \cite{hamming} is a similarity measure used for comparing binary vectors of the same length. It calculates the number of positions at which the corresponding elements of two vectors are different. The Hamming distance between two binary vectors $\mathbf{A}$ and $\mathbf{B}$ can be computed as:

$$
hamming\_distance(\mathbf{A}, \mathbf{B}) = \sum_{i=1}^{n} (A_i \oplus B_i)
$$

where $A_i$ and $B_i$ represent the $i$-th elements of vectors $\mathbf{A}$ and $\mathbf{B}$, respectively, and $\oplus$ denotes the bitwise XOR operation.

\subsection{Many LLMs}
Plethora of large language models (LLMs) are being released week upon week. Some models are made open-source : OpenLlama \cite{openllama}, GPT-2 \cite{gpt2}, others are proprietary : GPT-3\cite{gpt3} and GPT-4 \cite{gpt4}, some are published for commercial use such as Falcon \cite{falcon}, others are only for research purposes : Llama \cite{llama} and Vicuna \cite{vicuna}.
It's hard to benchmark the performance of these models due to the wide range of applicable evaluation tasks and the difficulty of filtering out the genuine progress from grandiose claims (as well as problems with current QA datasets and unrigorous evaluations \cite{unfairdataset}).
Huggingface recently published an LLM leaderboard \cite{open-llm-leaderboard}, where they evaluate publicly available LLMs on 4 key benchmarks : 

\begin{enumerate}
	\item AI2 Reasoning Challenge (25-shot) - a set of grade-school science questions \cite{AI2}.
	\item HellaSwag (10-shot) - a test of commonsense inference, which is easy for humans (~95\%) but challenging for SOTA models \cite{hellaswag}. 
	\item MMLU (5-shot) - a test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more \cite{MMLU}.
	\item TruthfulQA (0-shot) - a benchmark to measure whether a language model is truthful in generating answers to questions \cite{truthfulqa}.
\end{enumerate}

Most of the top performing models in this benchmark are either fine-tuned Falcon \cite{falcon} or fine-tuned LLama \cite{llama} models.

\paragraph*{Falcon}
Large language models are typically trained using a combination of carefully selected high-quality data sources and filtered web data. 
The curation process aims to create models that perform well across a wide range of tasks. 
However, as the size of models increases and more training data is required, concerns arise regarding the scalability of the curation process and the availability of unique high-quality data.

The refined web dataset for Falcon llm \cite{falcondataset} demonstrates that properly filtered and deduplicated web data alone can yield powerful language models, 
even outperform state-of-the-art models trained on curated datasets like The Pile \cite{pile}. 
The authors of this dataset\cite{falcondataset} have released the Falcon \cite{falcon} language models with 1.3/7.5 billion parameters trained on this dataset, 
which serve as valuable resources for this project.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.5\linewidth]{figures/falcon.png}
	\caption{Models trained on REFINEDWEB \cite{falcondataset} alone outperform models trained on curated corpora}
	\label{fig:falcon}
\end{figure}

\paragraph*{LLaMa}
The authors of \cite{llama} present LLaMA, a collection of foundation language models with parameters ranging from 7 billion to 65 billion. 
They demonstrate that it is possible to train state-of-the-art models using publicly available datasets alone, without relying on proprietary or inaccessible data. 
In particular, LLaMA-13B surpasses the performance of GPT-3 (175B) \cite{gpt3} on most benchmarks, and LLaMA-65B is on par with other top models like Chinchilla-70B \cite{chinchilla} and PaLM-540B \cite{palm}. 
By making open-sourcing the LLaMA architecture and training code and by making the weights available for researchers, plenty of other models appeared as a result of fine-tuning the original LLaMA models on various tasks and datasets, 
these fine-tuned models include Alpaca \cite{alpaca} and Vicuna \cite{vicuna}.     

\paragraph*{Open-LLaMA} \cite{openllama} provides a totally open-source replication (weights and code, no research-only restriction) of the original LLaMA \cite{llama} models and which surprisingly in some cases outperforms the original models according to their benchmark \cite{openllama}.

Open-LLaMA is a permissively licensed open source reproduction of LLaMA \cite{llama} 7B and 13B trained on the RedPajama \cite{redpajama} dataset. 

It was evaluated on a wide range of tasks using lm-evaluation-harness \cite{eval-harness} (results shown in table \ref*{tab:openllama}). 
The LLaMA \cite{llama} results are generated by running the original LLaMA model on the same evaluation metrics. 
They note that the results for the LLaMA model differ slightly from the original LLaMA paper, which can be due to different evaluation protocols. 
Additionally, they present the results of GPT-J \cite{gptj}, a 6B parameter model trained on the Pile dataset \cite{pile}.

The original LLaMA model was trained for 1 trillion tokens and GPT-J was trained for 500 billion tokens. from the results presented in table \ref*{tab:openllama} 
OpenLLaMA exhibits comparable performance to the original LLaMA \cite{llama} and GPT-J \cite{gptj} across a majority of tasks, and outperforms them in some tasks.

\begin{table}[h]
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|}
		\hline
		Task/Metric& GPT-J 6B & LLaMA 7B & LLaMA 13B & OpenLLaMA 7B & OpenLLaMA 3B & OpenLLaMA 13B \\ \hline
		anli\_r1/acc & 0.32 & 0.35 & 0.35 & 0.33 & 0.33 & 0.33 \\ \hline
		anli\_r2/acc & 0.34 & 0.34 & 0.36 & 0.36 & 0.32 & 0.33 \\ \hline
		anli\_r3/acc & 0.35 & 0.37 & 0.39 & 0.38 & 0.35 & 0.40 \\ \hline
		arc\_challenge/acc & 0.34 & 0.39 & 0.44 & 0.37 & 0.34 & 0.41 \\ \hline
		arc\_challenge/acc\_norm & 0.37 & 0.41 & 0.44 & 0.38 & 0.37 & 0.44 \\ \hline
		arc\_easy/acc & 0.67 & 0.68 & 0.75 & 0.72 & 0.69 & 0.75 \\ \hline
		arc\_easy/acc\_norm & 0.62 & 0.52 & 0.59 & 0.68 & 0.65 & 0.70 \\ \hline
		boolq/acc & 0.66 & 0.75 & 0.71 & 0.71 & 0.68 & 0.75 \\ \hline
		hellaswag/acc & 0.50 & 0.56 & 0.59 & 0.53 & 0.49 & 0.56 \\ \hline
		hellaswag/acc\_norm & 0.66 & 0.73 & 0.76 & 0.72 & 0.67 & 0.76 \\ \hline
		openbookqa/acc & 0.29 & 0.29 & 0.31 & 0.30 & 0.27 & 0.31 \\ \hline
		openbookqa/acc\_norm & 0.38 & 0.41 & 0.42 & 0.40 & 0.40 & 0.43 \\ \hline
		piqa/acc & 0.75 & 0.78 & 0.79 & 0.76 & 0.75 & 0.77 \\ \hline
		piqa/acc\_norm & 0.76 & 0.78 & 0.79 & 0.77 & 0.76 & 0.79 \\ \hline
		record/em & 0.88 & 0.91 & 0.92 & 0.89 & 0.88 & 0.91 \\ \hline
		record/f1 & 0.89 & 0.91 & 0.92 & 0.90 & 0.89 & 0.91 \\ \hline
		rte/acc & 0.54 & 0.56 & 0.69 & 0.60 & 0.58 & 0.64 \\ \hline
		truthfulqa\_mc/mc1 & 0.20 & 0.21 & 0.25 & 0.23 & 0.22 & 0.25 \\ \hline
		truthfulqa\_mc/mc2 & 0.36 & 0.34 & 0.40 & 0.35 & 0.35 & 0.38 \\ \hline
		wic/acc & 0.50 & 0.50 & 0.50 & 0.51 & 0.48 & 0.47 \\ \hline
		winogrande/acc & 0.64 & 0.68 & 0.70 & 0.67 & 0.62 & 0.70 \\ \hline
		Average & 0.52 & 0.55 & 0.57 & 0.55 & 0.53 & 0.57 \\ \hline
		\end{tabular}}
	\label{tab:openllama}
	\caption{Evaluation of OpenLlama on the lm-evaluation-harness comparing to LLaMa and GPT-J} 
\end{table}

\paragraph*{Vicuna} \cite{vicuna} is an open-source chatbot trained by fine-tuning LLaMA \cite{llama} on user-shared conversations collected from ShareGPT. The preliminary evaluation conducted using GPT-4 as a judge demonstrates that Vicuna-13B achieves more than 90\% 
quality compared to OpenAI's ChatGPT and Google Bard, surpassing models like LLaMA \cite{llama} and Stanford Alpaca \cite{alpaca} in over 90\% of cases. 
The cost of training Vicuna-13B is approximately \$300 (which is very impressive !). The code, weights, and an online demo of the chatbot are publicly available for non-commercial use.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.5\linewidth]{figures/vicuna.png}
	\caption{Response comparison of Vicuna assessed by GPT-4}
	\label{fig:vicuna}
\end{figure}


\subsection{Question answering}
\subsubsection{Open-domain QA}
Open-domain Question Answering (ODQA) is a
task examining the ability of models to produce answers to natural language factoid questions drawn
from an open set of domains.

DrQA \cite{drqa} implemented Wikipedia as its knowledge source and this choice has became a default setting for many ODQA studies since then.

However it's relevant in some cases (such as ours) to limit the knowledge source to a signal (or multiple) pre-defined documents, there is a slight difference between the two tasks, to avoid confusion we call the latter "document-based question answering". 

Nevertheless approaches in ODQA are applicable for DBQA.

\subsection{Methods for document-based question answering}
\subsubsection{Open-book QA}
Open-book models in the field of Open-Domain Question Answering (ODQA) initially retrieve relevant documents then either extract or generate answers based on the information contained in the retrieved documents. We can distinguish mainly two approaches to tackle the ODQA problem :

\begin{enumerate}
	\item Retriever-reader : this model works toward finding the related context in the documentation than process the retrieved context to \textbf{extract} the start/end positions of an answer. The output of the model is the selected context and the identified span of the answer in the context. 
	\item Retriever-generator : unlike the reader model, the generator model generate free text conditioned with the retrieved context to answer the question.
\end{enumerate}

A famous model following the retriever-reader approach is "Dense Passage Retrieval" (DPR) \cite{dpr}. DPR follows a pipeline approach, where documents are retrieved using dense embeddings. 
These retrieved documents are then passed to a conventional reader-re-ranker, which extracts specific spans of text as answers.

another example of a retriever-generator model is "Retrieval-Augmented Generation" \cite{rag} which is a seq2seq model that jointly learns to retrieve and generate answers. 
It utilizes dense retrieval and BART \cite{bart} for this purpose.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/openbook.png}
	\caption{Overview of open-book question answering approaches.}
	\label{fig:openbook}
\end{figure}

\subsubsection{Closed-book QA}
Large language models undergo extensive unsupervised pre-training phase using large amount of textual data. With a substantial number of parameters, these models possess the ability to memorize factual information within their weight parameters. Consequently, one is able to employ this property for question-answering tasks without relying on explicit context (shown in figure \ref{fig:closedbook}). 

The pre-trained language models generate free-form text responses to questions, without explicitly employing reading comprehension techniques.
Closed-book models can encode the given documentation within the parameters of the model itself to answer queries, rather than using a retrieval model.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/closedbook.png}
	\end{center}
	\caption{Overview of closed-book fine-tuned QA.}
	\label{fig:closedbook}
\end{figure}

Authors of \cite{T5Qa} fine-tune a pre-trained T5 \cite{T5} model to answer questions
(without access to any external context or knowledge) and were able (at the time) to compete with open-domain systems that
explicitly retrieve answers from an external knowledge source when answering questions.

GPT-3 \cite{gpt3} has been evaluated on the closed book question answering task using the TriviaQA dataset \cite{triviaqa} without any gradient updates or fine-tuning, the evaluation (figure \ref{fig:gpt3}) shows that GPT-3 match/exceed the performance of state of the art (at that time).

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/gpt3-triviaqa.png}
	\caption{GPT3's performance on TriviaQA \cite{triviaqa}. \cite{gpt3}}
	\label{fig:gpt3}
\end{figure}

\subsection{Retrieval models}
When it comes to implementing a retriever for a retriever-generator/retriever-reader models, there is mainly two systems : 
\begin{enumerate}
	\item using classic non-learning-based TF-IDF features (“classic IR”).
	\item or using dense embedding vectors of text produced by neural networks (“neural IR”).
\end{enumerate}

\paragraph{classic IR}
For example DrQA \cite{drqa} adopts an efficient non-learning-based search engine using bi-gram hashing
and TF-IDF matching.

Another approach used by BERTserini \cite{BERTserini} consists of ranking retrieved text segments using BM25 \cite{BM25}, a classic TF-IDF-based retrieval scoring function.

In terms of the effect of text granularity on performance, \cite{BERTserini} found that paragraph retrieval $>$ sentence retrieval $>$ article retrieval.

Multi-passage BERT QA model \cite{multibert} uses elasticSearch with BM25 \cite{BM25}. They found that splitting articles into passages with the length of 100 words by sliding window brings $4\%$ improvements, and that splitting documents into passages without overlap causes some near-boundary evidence to lose useful contexts thus decreasing the performance.

\paragraph{neural IR}
Neural IR is a new category of methods for retrieval problems, it mainly uses dense representations of some neural network architectures (e.g. LSTM \cite{lstm}, BERT \cite{bert}, etc). 

After the arrival of many general language models, many IR for QA systems started following this (or a slightly different) approach :
\begin{enumerate}
	\item Extract the dense representations of a question and a context passage by feeding them into a language model.
	\item Use the dot-product of these two representations as the retrieval score to rank and select most relevant passages.
\end{enumerate}

$$h_q=E_q(q)$$ 
$$h_z=E_z(z)$$
$$score=h_q^\mathsf{T} h_z$$

However write of \cite{lin} shows that it is not necessary that neural IR out-performs classic IR.

\subsection{Reader Models}
The reader model objective is to learn solve reading comprehension tasks, extract an answer for a question from a retrieved context. 
Here we only discuss approaches for machine comprehension using neural networks since it yields best performing results.

The reader model of DRQA \cite{drqa} is a 3-layer bidirectional LSTM with hidden size 128, it takes each paragraph in the document and represents it as a sequence of feature vectors. 
Each token in the paragraph is transformed into a feature vector, which includes the many components (GloVe \cite{glove} embedding, binary features for exact match, Manual features like part-of-speech (POS), named entity recognition (NER) tags, and term frequency, etc. ...).
The question is encoded using another recurrent neural network (RNN) applied to the word embeddings of each question word. The resulting hidden units are combined into a single vector that represents the question. At the paragraph level, the model aims to predict the most likely span of tokens that forms the correct answer. The paragraph vectors and the question vector are used as input. 
Two independent classifiers are trained to predict the start and end positions of the answer span.

Another example of neural reader models is found in BERTserini \cite{BERTserini}, which utilizes a pre-trained BERT model to work as the reader and shows high accuracy when evaluated on the SQuAD \cite{squad} dataset.
\clearpage
\section{Implementation and results}
\subsection{Introduction}
In our Implementation we chose a retriever-generator approach, we experimented with different combinations of embedding and Language models, as well as different similarity measures and text splitting strategies.

We evaluated the different systems using a sample from the SQuAD \cite{squad} dataset while using GPT-3 \cite{gpt3} as a scoring model (more about that in the sections below).  

\paragraph*{Similar work} LlamaIndex \cite{llamaindex} tested out few indexing methods (embeddings, LLMs and retrieval systems) following a similar framework to ours. 
However their focus was on OpenAI proprietary language models (GPT-3 and GPT-4) \cite{gpt3, gpt4}. Our focus in this project 
on the other hand is on using open-source embedding models and LLMs that can fully run on-premise, since model and data sovereignty are a critical aspect for our use-case.

In the following sections we will provide more details on our implementation as well as the evaluation results we got.

\subsection{challenges}
The following challenges arise when using LLMs for DBQA : 
\begin{enumerate}
	\item Small context window: LLMs typically have a limited context window, which means they can only consider a limited number of preceding tokens when generating a response. In DBQA, where the answer may depend on information scattered throughout a document, this limited context window can make it difficult for LLMs to capture the necessary context and provide accurate answers. Relevant information may fall outside the context window, leading to incomplete or incorrect responses. 
    This is why we need information retrieval systems when choosing the context.
	\item Hallucination: LLMs can sometimes generate responses that are plausible-sounding but factually incorrect. 
    This phenomenon, known as hallucination, can be a significant issue in DBQA. Since LLMs rely on statistical patterns and language modeling rather than true understanding, they may generate answers that sound reasonable but are not grounded in the actual content of the document. Hallucination can mislead users and compromise the reliability of the DBQA system.
	\item Storing the models and computational demand for inference: LLMs, especially large-scale models, require substantial computational resources for both training and inference. Storing these models and performing inference can be challenging due to their size and computational demands. These demands can pose practical challenges for deploying LLM-based DBQA systems at scale.
\end{enumerate}

We were able to workaround the first challenge using IR system based on neural-embedding. 
For the second challenge we tried using small temperature (a hyperparameter used to control the randomness of the generated text) and presence penalty values (high presence penalty values results in the model being more likely to 
generate tokens that have not yet been included in the generated text and vice-versa).
Using smaller models (7B parameters each) required around 36GB of memory, we ran the inferences on a multi-GPU machine.

\subsection{Description}
The systems that we experimented with follows the retriever-generator framework which consists mainly of two parts : 

\paragraph{A retrieval system} that takes as input a document (text) and a query from the user, splits the document into sentences. 
Each sentence from the document along the query of the user are going to be embedded by one of the chosen embedding models. We measure the one-to-one similarity (using cosine similarity) 
between the query embedding and each sentence embedding and we retrieve the top $K$ sentences to be used a context.

\paragraph{A conditioned generative system} it takes as input the retrieved contexts and the query to construct a prompt that condition the generation using one of the chosen open-source LLMs.
In table \ref{tab:benchmark} we show that we tested all the possible combinations of the different embedding models and LMs at hand.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.9\linewidth]{figures/full.png}
	\caption{Highlevel overview of the whole system.}
	\label{fig:overview}
\end{figure}

\subsection{Evaluation}
We bench-marked the different systems on the SQuAD dataset \cite{squad}. We sampled 20 examples from the dataset with their contexts and answers (examples can be found in annex \ref*{ann:experiments}), for each question and context 
we tested a different embedding model and LLM combination, each response is then evaluated using GPT-3 \cite{gpt3} (a score of +1 is given if the answer is correct 0 otherwise), results are shown in table \ref{tab:benchmark}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.9\linewidth]{figures/evaluation.png}
	\caption{Evaluation schema with GPT-3 \cite{gpt3}.}
	\label{fig:evaluation}
\end{figure}

\begin{table}[h]
	\centering
	\begin{tabular}{cccc}
		                                  &                 & \color{orange}{\textbf{Language model}} &                    \\
		\cline{2-4}
		\color{violet}{\textbf{Embedder}} & \textbf{Vicuna} & \textbf{Falcon}                         & \textbf{OpenLlama} \\
		\hline
		Bert                              & \textbf{0.4}    & 0.25                                    & 0.35               \\
		MiniLM                            & 0.35            & 0.25                                    & 0.3                \\
		E5                                & 0.35            & 0.25                                    & 0.3                \\
	\end{tabular}
	\caption{Benchmark : average score of each system when tested on 20 samples from SQuAD \cite{squad} dataset, and evaluated using GPT-3 \cite{gpt3}.}
	\label{tab:benchmark}
\end{table}

\subsection{Discussion}

\subsection{Implementation details}

In this section, we provide general details about the implementation of our benchmarking system. 
We also describe the specific setup used for our experiments.

\begin{enumerate}
	\item \textbf{Python version:} We implemented the system using Python 3.10.11, which provided us with the necessary language features and libraries for our project.
	\item \textbf{Operating system:} The implementation was developed and tested on Ubuntu 20.04.6 LTS x86\_64.
	\item \textbf{Graphics Processing Units (GPUs):} To leverage the power of parallel processing, our setup included a combination of GPUs. We utilized one NVIDIA GeForce GTX 1080 Ti and two NVIDIA GeForce RTX 3060 GPUs for faster inference.
	\item \textbf{Memory :} The system had access to a substantial amount of RAM, specifically 126GB. This allowed us to load the weights of the different LLMs easily.
\end{enumerate}

\subsubsection*{IR system details}
The Information Retrieval (IR) system employed a two-step process for document retrieval:

\begin{enumerate}
	\item \textbf{Splitting:} We tested different text splitting mechanisms, on the token level with overlap using the tiktoken package, 
    on the sentence level using a metaheuristic developed by the curators of Europarl \cite{europarl} (pseudo code available on annex \ref{ann:pseudocode}) and on the paragraph level by concatenating multiple sentences (while making sure that the last sentence ends with a dot).
	
    \item \textbf{Embedding:} We used a BERT \cite{bert} and E5 \cite{e5} from HuggingFace \cite{huggingface} (bert-base-cased-squad2 and e5-small respectively) and a MiniLM \cite{minilm} model from the SentenceTransformer package \cite{sbert}.
\end{enumerate}

During the evaluation phase we fix the text splitting to sentence level and the similarity measure to cosine similarity and $k=1$ (the number of retrieved context sentences), to make the evaluation a little more rigorous.  

\subsubsection*{Language models details}
We employed a batching strategy processing multiple input sequences simultaneously, reducing the overall processing time.

Using the OpenLlama project \cite{openllama} weights, we employed the LlamaForCausalLM model and the LlamaTokenizer from HuggingFace \cite{huggingface} to load and run the model. 
During generation, a maximum of 30 new tokens were allowed, and the generation temperature was set to 0.0 for deterministic output. 

For Falcon \cite{falcon} we employed the AutoTokenizer and the "text-generation" pipeline from the HuggingFace \cite{huggingface}. 
During generation, we used a temperature of $3x10^{-4}$. The maximum length of generated tokens was set to 30.

For Vicuna \cite{vicuna} we used FastChat \cite{fastchat} implementation.
We use their implementation for chat input and output as a workaround for batched inference (shown in annex \ref*{ann:vicuna}). 
The maximum number of new tokens and temperature are 30 and 0.0 respectively 

\subsubsection*{Evaluation details}
The evaluation of our benchmark involved sampling 20 random samples from the SQuAD \cite{squad} dataset, and using GPT-3 \cite{gpt3} as a scoring model.
We use LMQL \cite{lmql} programming framework to constrain the evaluation output of the scoring model (shown in annex \ref*{ann:evaluation}). LMQL uses token masking to prevent the generation of tokens that violate the specified constraints. 
If a constraint cannot be satisfied, the corresponding branch of generation is pruned, and the model continues generating in the remaining valid branches. This helps improve efficiency and constrain the model to output only necessary tokens 
(in our case it's 1 or 0).

\section{Conclusion}

% % Bibliography
\twocolumn
\bibliographystyle{plain}
\bibliography{sample_references}
\clearpage

% //////////////////////////////////////////////////////////////////////////////
% //////////////// Annexes /////////////////////////////////////////////////////
% //////////////////////////////////////////////////////////////////////////////

\onecolumn
\section{Annexes}

\subsection{Code}
\subsubsection{initializing the different LMs}
\begin{lstlisting}[language=Python]
from abc import ABC, abstractmethod

import gc

import torch
import transformers
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    LlamaForCausalLM,
    LlamaTokenizer
)


class LM(ABC):
    @abstractmethod
    def __call__(self, requests: list[str], **kwargs: any) -> list[str]:
        pass


class Llama(LM):
    def __init__(self, model_path: str = "models/openllama/7B") -> None:
        super().__init__()
        self.model_path = model_path
        self.tokenizer = LlamaTokenizer.from_pretrained(model_path)
        self.model = LlamaForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
        )

    def __call__(self, requests: list[str]) -> list[str]:
        outputs = []
        for prompt in requests:
            input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids
            generation_output = self.model.generate(
                input_ids=input_ids,
                max_new_tokens=30,
                temperature=0.0,
            )
            output = self.tokenizer.decode(
                generation_output[0], skip_special_tokens=True
            )
            output = output.replace(prompt, "")  # eq : return_full_sequence=False
            outputs.append(output)
            print(output)
        return outputs


class Falcon(LM):
    def __init__(self, model_path: str = "models/falcon/7B/snapshots/falcon") -> None:
        super().__init__()
        gc.collect()
        torch.cuda.empty_cache()
        self.model_name = model_path

    def __call__(self, requests: list[str]) -> list[str]:
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        pipeline_ = transformers.pipeline(
            "text-generation",
            model=self.model_name,
            tokenizer=tokenizer,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto",
        )
        sequences = pipeline_(
            requests,
            max_new_tokens=30,
            do_sample=True,
            top_k=10,
            temperature=3e-4,
            num_return_sequences=1,
            eos_token_id=tokenizer.eos_token_id,
            return_full_text=False,
        )
        outputs = [seq[0]["generated_text"] for seq in sequences]
        return outputs
\end{lstlisting}

\subsubsection{vicuna workaround}
\label{ann:vicuna}
\begin{lstlisting}[language=Python]
import torch
from fastchat.model.chatglm_model import chatglm_generate_stream
from fastchat.model.model_adapter import get_conversation_template, load_model
from fastchat.serve.inference import generate_stream

from src.lm import LM


class SimpleChatIO:
    """this is a workaround to use fastchat for batched inference"""

    def __init__(self, requests: list[str]) -> None:
        self.requests = requests

    def prompt_for_input(self) -> str:
        return self.requests.pop(0) if self.requests else ""

    def stream_output(self, output_stream):
        pre = 0
        for outputs in output_stream:
            output_text = outputs["text"]
            output_text = output_text.strip().split(" ")
            now = len(output_text) - 1
            if now > pre:
                print(" ".join(output_text[pre:now]), end=" ", flush=True)
                pre = now
        print(" ".join(output_text[pre:]), flush=True)
        return " ".join(output_text)


def chat_loop(
    model_path: str,
    device: str,
    num_gpus: int,
    max_gpu_memory: str,
    load_8bit: bool,
    cpu_offloading: bool,
    temperature: float,
    repetition_penalty: float,
    max_new_tokens: int,
    chatio: SimpleChatIO,
    debug: bool,
):
    # Model
    model, tokenizer = load_model(
        model_path, device, num_gpus, max_gpu_memory, load_8bit, cpu_offloading, debug
    )
    is_chatglm = "chatglm" in str(type(model)).lower()
    is_fastchat_t5 = "t5" in str(type(model)).lower()

    # Hardcode T5 repetition penalty to be 1.2
    if is_fastchat_t5 and repetition_penalty == 1.0:
        repetition_penalty = 1.2

    while True:
        conv = get_conversation_template(model_path)  # reset conversation

        try:
            inp = chatio.prompt_for_input()
        except EOFError:
            inp = ""
        if not inp:
            print("exit...")
            break

        conv.append_message(conv.roles[0], inp)
        conv.append_message(conv.roles[1], None)

        if is_chatglm:
            generate_stream_func = chatglm_generate_stream
            prompt = conv.messages[conv.offset :]
        else:
            generate_stream_func = generate_stream
            prompt = conv.get_prompt()

        gen_params = {
            "model": model_path,
            "prompt": prompt,
            "temperature": temperature,
            "repetition_penalty": repetition_penalty,
            "max_new_tokens": max_new_tokens,
            "stop": conv.stop_str,
            "stop_token_ids": conv.stop_token_ids,
            "echo": False,
        }

        output_stream = generate_stream_func(model, tokenizer, gen_params, device)
        outputs = chatio.stream_output(output_stream)

        yield outputs

        if debug:
            print("\n", {"prompt": prompt, "outputs": outputs}, "\n")


class Vicuna(LM):
    def __init__(self, model_path: str = "models/vicuna/7B") -> None:
        super().__init__()
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.num_gpus = 3
        self.max_gpu_memory = None
        self.load_8bit = False
        self.cpu_offloading = False
        self.max_new_tokens = 20

    def __call__(
        self,
        requests: list[str],
    ) -> list[str]:
        chatio = SimpleChatIO(requests=requests)
        outputs = [
            output
            for output in chat_loop(
                self.model_path,
                self.device,
                num_gpus=self.num_gpus,
                load_8bit=self.load_8bit,
                temperature=0.0,
                repetition_penalty=1.0,
                max_new_tokens=20,
                debug=False,
                chatio=chatio,
                cpu_offloading=self.cpu_offloading,
                conv_template=None,
                max_gpu_memory=None,
            )
        ]
        return outputs
\end{lstlisting}

\subsubsection{initializing the different embedding models}
\begin{lstlisting}[language=Python]
  from abc import ABC, abstractmethod

  import torch
  import torch.nn.functional as F
  from sentence_transformers import SentenceTransformer
  from torch import Tensor
  from transformers import AutoModel, AutoTokenizer, BertForQuestionAnswering
  
  from src.similarity_measure import CosineSimilarity
  
  
  class Embedder(ABC):
      @abstractmethod
      def embed(self, texts: list[str]) -> list[list[float]]:
          """Embed a list of contexts"""
          pass
  
      def embed_query(self, texts: list[str]) -> list[list[float]]:
          """Embed a list of queries"""
          return self.embed(texts)
  
      def embed_context(self, texts: list[str]) -> list[list[float]]:
          """Embed a list of contexts"""
          return self.embed(texts)
  
  
  class MiniLM(Embedder):
      def __init__(self) -> None:
          super().__init__()
          self.model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
  
      def embed(self, texts: list[str]) -> list[list[float]]:
          embeddings = self.model.encode(texts)
          return embeddings.tolist()
  
  
  class Bert(Embedder):
      def __init__(self) -> None:
          super().__init__()
          self.tokenizer = AutoTokenizer.from_pretrained("deepset/bert-base-cased-squad2")
          self.model = BertForQuestionAnswering.from_pretrained(
              "deepset/bert-base-cased-squad2"
          )
  
      def embed(self, texts: list[str]) -> list[list[float]]:
          inputs = [self.tokenizer(text, return_tensors="pt") for text in texts]
          with torch.no_grad():
              outputs = [
                  self.model(**input, output_hidden_states=True) for input in inputs
              ]
              # take the average of the last hidden-state of each token to represent the sentence
              outputs = [
                  output.hidden_states[-1].mean(dim=1).flatten().tolist()
                  for output in outputs
              ]
  
          return outputs
  
  
  class E5(Embedder):
      def __init__(self) -> None:
          super().__init__()
          self.tokenizer = AutoTokenizer.from_pretrained("intfloat/e5-small")
          self.model = AutoModel.from_pretrained("intfloat/e5-small")
  
      def average_pool(
          self, last_hidden_states: Tensor, attention_mask: Tensor
      ) -> Tensor:
          last_hidden = last_hidden_states.masked_fill(
              ~attention_mask[..., None].bool(), 0.0
          )
          return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
  
      def embed(self, texts: list[str]) -> list[list[float]]:
          batch_dict = self.tokenizer(texts, padding=True, return_tensors="pt")
  
          outputs = self.model(**batch_dict)
          embeddings = self.average_pool(
              outputs.last_hidden_state, batch_dict["attention_mask"]
          )
  
          # (Optionally) normalize embeddings
          embeddings = F.normalize(embeddings, p=2, dim=1)
          return embeddings.detach().tolist()
  
      def embed_query(self, texts: list[str]) -> list[list[float]]:
          return self.embed(["query :" + text for text in texts])
  
      def embed_context(self, texts: list[str]) -> list[list[float]]:
          return self.embed(["passage :" + text for text in texts])
\end{lstlisting}

\subsubsection{Evaluation using GPT-3 \cite{gpt3} with LMQL framework \cite{lmql}} 
\label{ann:evaluation}
\begin{lstlisting}[language=Python]
import os

import dotenv
import lmql
import openai

dotenv.load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")


@lmql.query
async def evaluate_gpt3(prediction: str, ground_truth: str):
    '''argmax
        """output 1 if the student answer is similar to true answer, 0 otherwise
        ignore small differences
        student answer: {prediction}
        true answer: {ground_truth}
        evaluation : [EVALUATION]"""
    from
        "openai/text-davinci-003"
    distribution
        EVALUATION in ["1", "0"]
    '''  
\end{lstlisting}

\subsubsection{Index construction}
\begin{lstlisting}[language=python]
from src.embedding import Embedder
from src.text_splitter import Splitter
from src.utils.utils import dump_pickle, read_pickle


class Index:
    def __init__(
        self,
        embedder: Embedder,
        splitter: Splitter,
    ) -> None:
        super().__init__()

        self.splitter = splitter
        self.embedder = embedder

    def __call__(self, document: str, index_path: str | None = None) -> list[str]:
        """returns an indexed document"""
        if document:
            self.index = index = self.index_document(document)
        elif index_path:
            self.index = index = self.load_index(index_path)
        else:
            raise ValueError("Either document or an index_path must be provided")

        return index

    def index_document(self, document: str) -> list[tuple]:
        chunks = self.splitter.split(document)
        embeddings = self.embedder.embed_context(chunks)
        return list(zip(chunks, embeddings))

    def save_index(self, path: str) -> None:
        assert self.index, "Index is empty"
        dump_pickle(self.index, path=path)

    def load_index(self, path: str) -> list[tuple]:
        return read_pickle(path=path)
\end{lstlisting}

\subsubsection{Sentence splitting heuristic \cite{europarl} pseudocode}
\label{ann:pseudocode}
\begin{lstlisting}
    function splitIntoSentences(text):
    sentences = []
    words = splitTextIntoWords(text)
    currentSentence = ""

    for i = 0 to length(words) - 1:
        word = words[i]
        nextWord = words[i + 1] if i + 1 < length(words) else ""

        if isEndOfSentence(word, nextWord):
            currentSentence += word
            sentences.append(currentSentence)
            currentSentence = ""
        else:
            currentSentence += word + " "

    return sentences

function isEndOfSentence(word, nextWord):
    if word ends with period, question mark, or exclamation mark:
        if nextWord starts with uppercase letter or sentence starter punctuation:
            return true

    if word ends with multiple consecutive dots:
        if nextWord starts with uppercase letter or sentence starter punctuation:
            return true

    if word ends with punctuation inside quotes or parentheses:
        if nextWord starts with possible sentence starter punctuation and uppercase letter:
            return true

    if isHonorific(word) and nextWord is empty:
        return false

    if isUppercaseAcronym(word):
        return false

    if word ends with period:
        if nextWord starts with uppercase letter or sentence starter punctuation:
            return true

    return false

function isHonorific(word):
    if word is a known honorific:
        return true
    return false

function isUppercaseAcronym(word):
    if word consists of uppercase letters and ends with period:
        return true
    return false

function splitTextIntoWords(text):
    words = []
    currentWord = ""
    
    for character in text:
        if character is whitespace:
            if currentWord is not empty:
                words.append(currentWord)
                currentWord = ""
        else:
            currentWord += character
    
    if currentWord is not empty:
        words.append(currentWord)
    
    return words
\end{lstlisting}

\subsection{Experiments}
\label{ann:experiments}
\subsubsection*{Results sample}
\begin{table}[htbp]
    \centering
    \begin{tabular}{|p{3cm}|p{7cm}|}
    \hline
    Question & What path does a ball thrown up and down in a moving vehicle take when seen by an outside observer? \\
    \hline
    Context & A person can throw a ball straight up in the air and catch it as it falls down without worrying about applying a force in the direction the vehicle is moving. \\
    \hline
    True Answer & parabolic \\
    \hline
    Vicuna Answer & When seen by an outside observer, a ball thrown up and down in a moving vehicle takes a cur \\
    \hline
    Llama Answer & The ball is thrown up and falls down.\newline \newline A: The ball is thrown up and falls down.\newline \newline A: The ball is thrown \\
    \hline
    Falcon Answer & The ball will move in a straight line.\newline Question: What is the direction of the ball's motion when seen by an outside observer? \\
    \hline
    \end{tabular}
\end{table}
    
\begin{table}[htbp]
    \centering
    \begin{tabular}{|p{3cm}|p{7cm}|}
    \hline
    Question & A small fraction of the cold water flow from Lake Constance goes to what other lake? \\
    \hline
    Context & A small fraction of the flow is diverted off the island of Mainau into Lake Überlingen. \\
    \hline
    True Answer & Lake Überlingen \\
    \hline
    Vicuna Answer & A small fraction of the cold water flow from Lake Constance goes to Lake Überlingen. \\
    \hline
    Llama Answer & Lake Überlingen \\
    \hline
    Falcon Answer & Lake Überlingen 
    Question: What is the name of the island in Lake Constance? 
    Answer: Mainau
    Question: What is the name of... \\
    \hline
    \end{tabular}
    \end{table}

    \begin{table}[htbp]
        \centering
        \begin{tabular}{|p{3cm}|p{7cm}|}
        \hline
        Question & The first episode of Doctor Who premiered the day after what famous event in history? \\
        \hline
        Context & Premiering the day after the assassination of John F. Kennedy, the first episode of Doctor Who was repeated with the second episode the following week. \\
        \hline
        True Answer & The assassination of John F. Kennedy \\
        \hline
        Vicuna Answer & The first episode of Doctor Who premiered the day after the assassination of John F. Kennedy. \\
        \hline
        Llama Answer & The assassination of John F. Kennedy \\
        \hline
        Falcon Answer & The first episode of Doctor Who premiered the day after the assassination of John F. Kennedy.
        The first episode of Doctor Who premiered on November 23... \\
        \hline
        \end{tabular}
    \end{table}
    
    \begin{table}[htbp]
        \centering
        \begin{tabular}{|p{3cm}|p{7cm}|}
        \hline
        \textbf{Row} & \textbf{Value} \\
        \hline
        Question & The merger between ITT and ABC was suspended after a complaint was filed by whom in July 1967? \\
        \hline
        Context & The merger was suspended, and a complaint was filed by the Department of Justice in July 1967, with ITT going to trial in October 1967; the merger was officially canceled after the trial's conclusion on January 1, 1968. \\
        \hline
        True Answer & Department of Justice \\
        \hline
        Vicuna Answer & The merger between ITT and ABC was suspended after a complaint was filed by the Department of Justice. \\
        \hline
        Llama Answer & The merger between ITT and ABC was suspended after a complaint was filed by the Department of Justice in July 1967, with ITT... \\
        \hline
        Falcon Answer & The merger was suspended, and a complaint was filed by the Department of Justice in July 1967.The merger between ITT and ABC was... \\
        \hline
        \end{tabular}
        \end{table}
        

\begin{lstlisting}[language=Python]
\end{lstlisting}
\end{document}
